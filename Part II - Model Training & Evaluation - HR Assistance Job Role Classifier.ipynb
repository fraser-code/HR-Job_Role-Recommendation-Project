{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6b186b",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "\n",
    "## Collaborative Initiative with the HR Department of an IT Company\n",
    "\n",
    "### Objective\n",
    "\n",
    "In collaboration with the HR department of a leading IT company, our goal is to optimize and streamline the hiring process. The company is actively recruiting for various technical roles, each requiring distinct skill sets.\n",
    "\n",
    "### Targeted Roles for Recruitment:\n",
    "\n",
    "- **Back-End Developers**\n",
    "- **Full-Stack Developers**\n",
    "- **Mobile Developers**\n",
    "- **Data Scientists & Machine Learning Specialists**\n",
    "- **Data Engineers**\n",
    "\n",
    "### Challenge\n",
    "\n",
    "The Talent Acquisition team collects extensive data from application forms, detailing the skills of candidates applying for different roles. However, candidates may express interest in positions that do not align with their actual skill sets, or they might be better suited for a different role than they initially selected. This misalignment creates inefficiencies in the hiring process.\n",
    "\n",
    "### Solution\n",
    "\n",
    "To address this challenge, we are developing a **machine learning model** that will analyze candidate data and accurately predict the most suitable job role for each applicant. This predictive approach will:\n",
    "\n",
    "- Improve **candidate-role matching accuracy**  \n",
    "- **Reduce hiring time** by prioritizing best-fit candidates  \n",
    "- **Optimize resource allocation** during the recruitment process  \n",
    "\n",
    "By integrating this model into the hiring pipeline, we aim to enhance efficiency, ensure better placements, and support data-driven decision-making in talent acquisition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55bc919",
   "metadata": {},
   "source": [
    "## Importing essential libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f78018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: upgrade: not found\n",
      "Requirement already satisfied: imbalanced-learn in /opt/venv/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /opt/venv/lib/python3.10/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /opt/venv/lib/python3.10/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /opt/venv/lib/python3.10/site-packages (from imbalanced-learn) (2.0.1)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /opt/venv/lib/python3.10/site-packages (from imbalanced-learn) (1.14.0)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /opt/venv/lib/python3.10/site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /opt/venv/lib/python3.10/site-packages (from imbalanced-learn) (0.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /opt/venv/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/venv/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/venv/lib/python3.10/site-packages (from scikit-learn) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/venv/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/venv/lib/python3.10/site-packages (from scikit-learn) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: xgboost in /opt/venv/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: scipy in /opt/venv/lib/python3.10/site-packages (from xgboost) (1.14.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /opt/venv/lib/python3.10/site-packages (from xgboost) (2.25.1)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.10/site-packages (from xgboost) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: kmodes in /opt/venv/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in /opt/venv/lib/python3.10/site-packages (from kmodes) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/venv/lib/python3.10/site-packages (from kmodes) (1.4.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in /opt/venv/lib/python3.10/site-packages (from kmodes) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/venv/lib/python3.10/site-packages (from kmodes) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/venv/lib/python3.10/site-packages (from scikit-learn>=0.22.0->kmodes) (3.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: joblib in /opt/venv/lib/python3.10/site-packages (1.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in /opt/venv/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/venv/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/venv/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/venv/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/venv/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/venv/lib/python3.10/site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: stack-data in /opt/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /opt/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.12.2)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: decorator in /opt/venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/venv/lib/python3.10/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/venv/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: IPython in /opt/venv/lib/python3.10/site-packages (8.26.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /opt/venv/lib/python3.10/site-packages (from IPython) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup in /opt/venv/lib/python3.10/site-packages (from IPython) (1.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/venv/lib/python3.10/site-packages (from IPython) (4.9.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/venv/lib/python3.10/site-packages (from IPython) (0.19.1)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/venv/lib/python3.10/site-packages (from IPython) (5.14.3)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/venv/lib/python3.10/site-packages (from IPython) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/venv/lib/python3.10/site-packages (from IPython) (0.6.3)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/venv/lib/python3.10/site-packages (from IPython) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/venv/lib/python3.10/site-packages (from IPython) (3.0.47)\n",
      "Requirement already satisfied: decorator in /opt/venv/lib/python3.10/site-packages (from IPython) (5.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/venv/lib/python3.10/site-packages (from jedi>=0.16->IPython) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/venv/lib/python3.10/site-packages (from pexpect>4.3->IPython) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/venv/lib/python3.10/site-packages (from stack-data->IPython) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/venv/lib/python3.10/site-packages (from stack-data->IPython) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/venv/lib/python3.10/site-packages (from stack-data->IPython) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/venv/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->IPython) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!upgrade pip\n",
    "!pip install imbalanced-learn\n",
    "!pip install --upgrade scikit-learn\n",
    "!pip install xgboost\n",
    "!pip install kmodes\n",
    "!pip install joblib\n",
    "!pip install ipywidgets\n",
    "!pip install IPython\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from kmodes.kmodes import KModes\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score, f1_score, make_scorer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182b0a2",
   "metadata": {},
   "source": [
    "## Importing Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ea9aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cleaned_data/CleanedData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3d8573",
   "metadata": {},
   "source": [
    "## Data Prepration\n",
    "Majority of the cleaning has been completed in Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b868fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for important columns\n",
    "df = data[['LanguageHaveWorkedWith', 'DatabaseHaveWorkedWith', 'PlatformHaveWorkedWith', \n",
    "           'WebframeHaveWorkedWith', 'MiscTechHaveWorkedWith', 'ToolsTechHaveWorkedWith', \n",
    "           'OfficeStackAsyncHaveWorkedWith', 'NEWCollabToolsHaveWorkedWith', 'OfficeStackSyncHaveWorkedWith',\n",
    "           'DevType']].dropna(subset=['LanguageHaveWorkedWith']).copy()\n",
    "\n",
    "list_cols = ['LanguageHaveWorkedWith', 'DatabaseHaveWorkedWith', 'PlatformHaveWorkedWith', \n",
    "             'WebframeHaveWorkedWith', 'MiscTechHaveWorkedWith', 'ToolsTechHaveWorkedWith', \n",
    "             'OfficeStackAsyncHaveWorkedWith', 'NEWCollabToolsHaveWorkedWith', 'OfficeStackSyncHaveWorkedWith']\n",
    "\n",
    "\n",
    "# Splitting List columns using Multi Label Binaizer\n",
    "for i in list_cols:\n",
    "    df[i] = df[i].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    transformed = mlb.fit_transform(df[i])\n",
    "    binarized_df = pd.DataFrame(transformed, columns=mlb.classes_, index=df.index)\n",
    "    \n",
    "    df = df.drop(columns=[i])\n",
    "    df = pd.concat([df, binarized_df], axis=1)\n",
    "    \n",
    "    \n",
    "# Filtering for roles we are looking to hire for\n",
    "dev_types = [\n",
    "    'Back-End Developer', \n",
    "    'Full-Stack Developer',\n",
    "    'Mobile Developer',\n",
    "    'Data Engineer',\n",
    "    'Data Scientist/ML Specialist'\n",
    "    ]\n",
    "\n",
    "df = df[df['DevType'].isin(dev_types)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d2b7ad",
   "metadata": {},
   "source": [
    "### Performing a Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "993c5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent and dependent variables\n",
    "X = df.drop(columns=[\"DevType\"])\n",
    "y = df[\"DevType\"]\n",
    "\n",
    "# Split data before applying SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7826fc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DevType\n",
       "Full-Stack Developer            13610\n",
       "Back-End Developer               7611\n",
       "Mobile Developer                 1310\n",
       "Data Scientist/ML Specialist      784\n",
       "Data Engineer                     720\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e96dcf",
   "metadata": {},
   "source": [
    "### Using SMOTE to Oversample Minoroties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "882c5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the training data\n",
    "\n",
    "def apply_smote(X_train, y_train, threshold=3000):\n",
    "    value_counts = y_train.value_counts()\n",
    "    categories_to_oversample = value_counts[value_counts < threshold].index\n",
    "    smote = SMOTE(sampling_strategy={cat: threshold for cat in categories_to_oversample}, random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "X_train_resampled, y_train_resampled = apply_smote(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7858ac4",
   "metadata": {},
   "source": [
    "### Apply K-Modes Clustering for Feature Engineering"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35c9ddb2",
   "metadata": {},
   "source": [
    "# Determining best value for k using Elbow method \n",
    "k_values = range(2, 11)\n",
    "costs = []\n",
    "\n",
    "for k in k_values:\n",
    "    km = KModes(n_clusters=k, init='Huang', n_init=10, verbose=0, random_state=42)\n",
    "    km.fit(X_train_resampled)\n",
    "    costs.append(km.cost_)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_values, costs, marker='o', linestyle='dashed', color='b')\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Cost (Sum of Dissimilarities)\")\n",
    "plt.title(\"Elbow Method for Optimal k in K-Modes\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05c9b27",
   "metadata": {},
   "source": [
    "![Elbow Method](images/Elbow.png)\n",
    "In this case, the elbow seems to be around k = 3 or 4. We will consider 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3f4312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#km_4 = KModes(n_clusters=4, init='Huang', n_init=10, verbose=0, random_state=42)\n",
    "km_4 = joblib.load(\"models/km_4.pkl\")\n",
    "#clusters_train = km_4.fit_predict(X_train_resampled)\n",
    "clusters_train = km_4.predict(X_train_resampled)\n",
    "clusters_test = km_4.predict(X_test)\n",
    "#joblib.dump(km_4, \"models/km_4.pkl\")\n",
    "#joblib.dump(km_4,\"models/km_4.joblib\")\n",
    "\n",
    "# Add cluster column to datasets\n",
    "X_train_resampled[\"Cluster\"] = clusters_train\n",
    "X_test[\"Cluster\"] = clusters_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb714d2a",
   "metadata": {},
   "source": [
    "### Extracting Important Feaures using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4eae471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier to extract feature importance\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "rfc.fit(X_train_resampled, y_train_resampled)\n",
    "feature_importances = pd.Series(rfc.feature_importances_, index=X_train_resampled.columns)\n",
    "\n",
    "# Eliminate low significance features\n",
    "important_features = feature_importances[feature_importances > 0.0001].index  # Adjust threshold as needed\n",
    "X_train_filtered = X_train_resampled[important_features]\n",
    "X_test_filtered = X_test[important_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79eeb84",
   "metadata": {},
   "source": [
    "### One Hot Encoding Our Cluster Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c3d4a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply One-Hot Encoding for Cluster\n",
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_clusters_train = ohe.fit_transform(X_train_filtered[[\"Cluster\"]])\n",
    "encoded_clusters_test = ohe.transform(X_test_filtered[[\"Cluster\"]])\n",
    "\n",
    "# Convert to DataFrame with correct index\n",
    "encoded_cluster_columns = ohe.get_feature_names_out([\"Cluster\"])\n",
    "encoded_cluster_df_train = pd.DataFrame(encoded_clusters_train, columns=encoded_cluster_columns, index=X_train_filtered.index)\n",
    "encoded_cluster_df_test = pd.DataFrame(encoded_clusters_test, columns=encoded_cluster_columns, index=X_test_filtered.index)\n",
    "\n",
    "X_train_linear = X_train_filtered.drop(columns=[\"Cluster\"])\n",
    "X_test_linear = X_test_filtered.drop(columns=[\"Cluster\"])\n",
    "\n",
    "X_train_linear = pd.concat([X_train_linear, encoded_cluster_df_train], axis=1)\n",
    "X_test_linear = pd.concat([X_test_linear, encoded_cluster_df_test], axis=1)\n",
    "X_test_linear = X_test_linear.reindex(columns=X_train_linear.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149dd05",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7363080c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression(SMOTE) Accuracy: 0.7602\n",
      "Logistic Regression Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "          Back-End Developer       0.71      0.60      0.65      1903\n",
      "               Data Engineer       0.43      0.50      0.46       180\n",
      "Data Scientist/ML Specialist       0.62      0.69      0.65       196\n",
      "        Full-Stack Developer       0.80      0.86      0.83      3403\n",
      "            Mobile Developer       0.80      0.82      0.81       327\n",
      "\n",
      "                    accuracy                           0.76      6009\n",
      "                   macro avg       0.67      0.69      0.68      6009\n",
      "                weighted avg       0.76      0.76      0.76      6009\n",
      "\n",
      "\n",
      "SVC (SMOTE) Accuracy: 0.7665\n",
      "SVC Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "          Back-End Developer       0.73      0.58      0.65      1903\n",
      "               Data Engineer       0.46      0.37      0.41       180\n",
      "Data Scientist/ML Specialist       0.68      0.66      0.67       196\n",
      "        Full-Stack Developer       0.79      0.90      0.84      3403\n",
      "            Mobile Developer       0.86      0.80      0.83       327\n",
      "\n",
      "                    accuracy                           0.77      6009\n",
      "                   macro avg       0.70      0.66      0.68      6009\n",
      "                weighted avg       0.76      0.77      0.76      6009\n",
      "\n",
      "\n",
      "Gradient Boosting (SMOTE) Accuracy: 0.7589\n",
      "Gradient Boosting Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "          Back-End Developer       0.73      0.57      0.64      1903\n",
      "               Data Engineer       0.39      0.38      0.38       180\n",
      "Data Scientist/ML Specialist       0.61      0.75      0.67       196\n",
      "        Full-Stack Developer       0.79      0.88      0.83      3403\n",
      "            Mobile Developer       0.79      0.81      0.80       327\n",
      "\n",
      "                    accuracy                           0.76      6009\n",
      "                   macro avg       0.66      0.68      0.67      6009\n",
      "                weighted avg       0.76      0.76      0.75      6009\n",
      "\n",
      "\n",
      "Random Forest (SMOTE) Accuracy: 0.7544\n",
      "Random Forest Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "          Back-End Developer       0.74      0.53      0.62      1903\n",
      "               Data Engineer       0.43      0.28      0.34       180\n",
      "Data Scientist/ML Specialist       0.63      0.69      0.66       196\n",
      "        Full-Stack Developer       0.77      0.91      0.83      3403\n",
      "            Mobile Developer       0.83      0.74      0.78       327\n",
      "\n",
      "                    accuracy                           0.75      6009\n",
      "                   macro avg       0.68      0.63      0.65      6009\n",
      "                weighted avg       0.75      0.75      0.74      6009\n",
      "\n",
      "\n",
      "XGBoost(SMOTE) Accuracy: 0.7615\n",
      "XGBoost Classification Report:\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "          Back-End Developer       0.72      0.60      0.65      1903\n",
      "               Data Engineer       0.40      0.34      0.37       180\n",
      "Data Scientist/ML Specialist       0.66      0.68      0.67       196\n",
      "        Full-Stack Developer       0.79      0.88      0.83      3403\n",
      "            Mobile Developer       0.83      0.80      0.82       327\n",
      "\n",
      "                    accuracy                           0.76      6009\n",
      "                   macro avg       0.68      0.66      0.67      6009\n",
      "                weighted avg       0.76      0.76      0.76      6009\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['xgb_smoted.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training Logistic Regression Model\n",
    "logreg_smoted = LogisticRegression(n_jobs=-1, random_state=42)\n",
    "logreg_smoted.fit(X_train_linear, y_train_resampled)\n",
    "y_pred_logreg = logreg_smoted.predict(X_test_linear)\n",
    "\n",
    "#Saving Our Model For further use\n",
    "joblib.dump(logreg_smoted, \"models/logreg_smoted.joblib\")\n",
    "\n",
    "#Model Evaluation\n",
    "print(f\"Logistic Regression(SMOTE) Accuracy: {accuracy_score(y_test, y_pred_logreg):.4f}\")\n",
    "print(f\"Logistic Regression Classification Report:\\n{classification_report(y_test, y_pred_logreg)}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "### Training Support Vector Classifier Model\n",
    "svc_smoted = SVC(random_state=42)\n",
    "svc_smoted.fit(X_train_linear, y_train_resampled)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred_svc = svc_smoted.predict(X_test_linear)\n",
    "print(f\"SVC (SMOTE) Accuracy: {accuracy_score(y_test, y_pred_svc):.4f}\")\n",
    "print(f\"SVC Classification Report:\\n{classification_report(y_test, y_pred_svc)}\\n\")\n",
    "\n",
    "#Saving Our Model For further use\n",
    "joblib.dump(svc_smoted, \"models/svc_smoted.joblib\")\n",
    "\n",
    "\n",
    "\n",
    "# Training Gradient Boosting Classifier\n",
    "X_train_gb = X_train_filtered.copy()\n",
    "X_test_gb = X_test_filtered.copy()\n",
    "gb_smoted = GradientBoostingClassifier(random_state = 42)\n",
    "gb_smoted.fit(X_train_gb, y_train_resampled)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred_gb = gb_smoted.predict(X_test_gb)\n",
    "print(f\"Gradient Boosting (SMOTE) Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}\")\n",
    "print(f\"Gradient Boosting Classification Report:\\n{classification_report(y_test, y_pred_gb)}\\n\")\n",
    "\n",
    "#Saving Our Model For further use\n",
    "joblib.dump(gb_smoted, \"models/gb_smoted.joblib\")\n",
    "\n",
    "\n",
    "\n",
    "#Training Random Forest Classifier Model\n",
    "X_train_rfc = X_train_filtered.copy()\n",
    "X_test_rfc = X_test_filtered.copy()\n",
    "rfc_smoted = RandomForestClassifier(n_jobs = -1, random_state = 42)\n",
    "rfc_smoted.fit(X_train_rfc, y_train_resampled)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred_rfc = rfc_smoted.predict(X_test_rfc)\n",
    "print(f\"Random Forest (SMOTE) Accuracy: {accuracy_score(y_test, y_pred_rfc):.4f}\")\n",
    "print(f\"Random Forest Classification Report:\\n{classification_report(y_test, y_pred_rfc)}\\n\")\n",
    "\n",
    "#Saving Our Model For further use\n",
    "joblib.dump(rfc_smoted, \"models/rfc_smoted.joblib\")\n",
    "\n",
    "\n",
    "\n",
    "# Training XGB model\n",
    "X_train_xgb = X_train_filtered.copy()\n",
    "X_test_xgb = X_test_filtered.copy()\n",
    "\n",
    "# Applying Label encoding to our target variable\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train_resampled)\n",
    "y_test_encoded = le.transform(y_test)\n",
    "\n",
    "xgb_smoted = XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1)\n",
    "xgb_smoted.fit(X_train_xgb, y_train_encoded)\n",
    "y_pred_xgb = xgb_smoted.predict(X_test_xgb)\n",
    "\n",
    "#Model Evaluation\n",
    "y_pred_xgb_original = le.inverse_transform(y_pred_xgb)\n",
    "y_test_original = le.inverse_transform(y_test_encoded)\n",
    "print(f\"XGBoost(SMOTE) Accuracy: {accuracy_score(y_test_original, y_pred_xgb_original):.4f}\")\n",
    "print(f\"XGBoost Classification Report:\\n{classification_report(y_test_original, y_pred_xgb_original)}\\n\")\n",
    "\n",
    "#Saving Our Model For further use\n",
    "joblib.dump(xgb_smoted, \"models/xgb_smoted.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf9c03d",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "In this project I have trained multiple models on various ML algorithms, namely Logistic Regression, SVC, Gradient Boosting, XG Boost and Random Forest Classifier using a handful of ways to optimise the results. However, there is no one model fits all among them. There is a significant variance in their performance for each class. Selecting a model really depends on the hiring requirements. Given that, a model can be selected depending on the precision and recall and various other evaluation metrics.\n",
    "\n",
    "I'm picking the Logistic Regression model. It has been aided by SMOTE applied to balance classes in the dataset and K-Modes clustering for feature engineering based on similarities. The reason I have selected Logistic Regression is because of it's better recall for minoroty classes like Data Scientists and Data Engineers. The count of Data Scientists and Data Engineers is fairly low as is and were trying to identify as many as possible. We are able to detect 50% of Data Engineers and 69% of Data Scientists as indicated by the recall scores of the Logistic Regression Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59326f2d",
   "metadata": {},
   "source": [
    "**Note:** If the widgets do not show on first attempt, exit the notebook and reopen it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b27cf9dc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb07bc39ec04fd98bc7c708502318b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='LanguageHaveWorkedWith'), Checkbox(value=False, description='APL'), Checkbox(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14bfebb9efe46ef8f4aa9a04c1cc55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='DatabaseHaveWorkedWith'), Checkbox(value=False, description='BigQuery'), Checkbox(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a36313656340df964e288844ef5d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='PlatformHaveWorkedWith'), Checkbox(value=False, description='Amazon Web Services (…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa4fca2b9f6426d98ec09109f7cbd36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='WebframeHaveWorkedWith'), Checkbox(value=False, description='ASP.NET'), Checkbox(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ca294a1f604e64ad31505b9811cbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='MiscTechHaveWorkedWith'), Checkbox(value=False, description='.NET (5+) '), Checkbo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711bf8bf4c20425ea87de9c110195b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='ToolsTechHaveWorkedWith'), Checkbox(value=False, description='APT'), Checkbox(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e47f812a1e4d8998898d105bd60157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='OfficeStackAsyncHaveWorkedWith'), Checkbox(value=False, description='Adobe Workfro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c3c4589ff8e404a9fbe22216c30ed13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='NEWCollabToolsHaveWorkedWith'), Checkbox(value=False, description='Android Studio'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf02107a2f384d22adeeef5771fe2c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='OfficeStackSyncHaveWorkedWith'), Checkbox(value=False, description='Cisco Webex Te…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22971b6dbde84913a897ce64928a4783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Predict Role', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Developer Role: Data Scientist/ML Specialist\n"
     ]
    }
   ],
   "source": [
    "# Load trained model\n",
    "model = joblib.load(f\"models/logreg_smoted.joblib\")\n",
    "\n",
    "# Set column names used in training\n",
    "important_features = X_train_linear\n",
    "\n",
    "# Create UI elements\n",
    "widgets_list = {}\n",
    "user_input = {}\n",
    "categories = {}\n",
    "\n",
    "# Convert string lists to Python lists\n",
    "for col in list_cols:\n",
    "    data[col] = data[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    transformed = mlb.fit_transform(data[col])\n",
    "    transformed_df = pd.DataFrame(transformed, columns=mlb.classes_)\n",
    "    \n",
    "    categories[col] = list(transformed_df.columns)\n",
    "\n",
    "def predict_role(change):\n",
    "    # Convert user selections to DataFrame\n",
    "    global input_df\n",
    "    input_data = {col: 0 for col in X_train_resampled}\n",
    "\n",
    "    for category, checkboxes in widgets_list.items():\n",
    "        for tech, checkbox in checkboxes.items():\n",
    "            if checkbox.value:\n",
    "                if tech in input_data:\n",
    "                    input_data[tech] = 1\n",
    "\n",
    "    input_df = pd.DataFrame([input_data])\n",
    "    input_df = input_df.reindex(columns=X_train_resampled.columns, fill_value=0)\n",
    "    # Apply clustering and one hot encode the cluster column\n",
    "    cluster_prediction = km_4.predict(input_df.drop(columns=['Cluster'], errors='ignore'))\n",
    "    cluster_encoded = ohe.transform([[cluster_prediction[0]]])\n",
    "    global cluster_encoded_df\n",
    "    cluster_encoded_df = pd.DataFrame(cluster_encoded, columns=encoded_cluster_columns, index=input_df.index)\n",
    "    input_df = pd.concat([input_df.drop(columns=['Cluster'], errors='ignore'), cluster_encoded_df], axis=1)\n",
    "    input_df = input_df[X_train_linear.columns]\n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_df)[0]\n",
    "    print(f\"Predicted Developer Role: {prediction}\")\n",
    "\n",
    "\n",
    "# Creating checkboxes for each category\n",
    "category_boxes = []\n",
    "for category, options in categories.items():\n",
    "    checkboxes = {option: widgets.Checkbox(value=False, description=option) for option in options}\n",
    "    widgets_list[category] = checkboxes\n",
    "    category_box = widgets.VBox([widgets.Label(category)] + list(checkboxes.values()))\n",
    "    category_boxes.append(category_box)\n",
    "\n",
    "# Displaying checkboxes\n",
    "display(*category_boxes)\n",
    "\n",
    "# Button for prediction\n",
    "predict_button = widgets.Button(description=\"Predict Role\")\n",
    "predict_button.on_click(predict_role)\n",
    "display(predict_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace2ffa0",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning for RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db2dd1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"DevType\"])\n",
    "y = df[\"DevType\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fb5d9e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=150; total time=   4.9s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=150; total time=   4.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=2, n_estimators=150; total time=   4.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=150; total time=   4.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=150; total time=   4.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=5, n_estimators=150; total time=   4.3s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time=   3.5s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time=   3.5s\n",
      "[CV] END max_depth=30, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time=   3.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=5, min_samples_split=2, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=5, min_samples_split=2, n_estimators=150; total time=   3.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=5, min_samples_split=2, n_estimators=150; total time=   3.3s\n",
      "[CV] END max_depth=30, min_samples_leaf=5, min_samples_split=5, n_estimators=150; total time=   3.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=5, min_samples_split=5, n_estimators=150; total time=   3.1s\n",
      "[CV] END max_depth=30, min_samples_leaf=5, min_samples_split=5, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=5, min_samples_split=10, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=5, min_samples_split=10, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=30, min_samples_leaf=5, min_samples_split=10, n_estimators=150; total time=   3.4s\n",
      "[CV] END max_depth=30, min_samples_leaf=10, min_samples_split=2, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=10, min_samples_split=2, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=10, min_samples_split=2, n_estimators=150; total time=   2.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=   2.6s\n",
      "[CV] END max_depth=30, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=   2.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=10, min_samples_split=10, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=30, min_samples_leaf=10, min_samples_split=10, n_estimators=150; total time=   2.8s\n",
      "[CV] END max_depth=30, min_samples_leaf=10, min_samples_split=10, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=35, min_samples_leaf=2, min_samples_split=2, n_estimators=150; total time=   4.2s\n",
      "[CV] END max_depth=35, min_samples_leaf=2, min_samples_split=2, n_estimators=150; total time=   4.4s\n",
      "[CV] END max_depth=35, min_samples_leaf=2, min_samples_split=2, n_estimators=150; total time=   4.6s\n",
      "[CV] END max_depth=35, min_samples_leaf=2, min_samples_split=5, n_estimators=150; total time=   4.1s\n",
      "[CV] END max_depth=35, min_samples_leaf=2, min_samples_split=5, n_estimators=150; total time=   4.0s\n",
      "[CV] END max_depth=35, min_samples_leaf=2, min_samples_split=5, n_estimators=150; total time=   3.9s\n",
      "[CV] END max_depth=35, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time=   3.7s\n",
      "[CV] END max_depth=35, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time=   3.6s\n",
      "[CV] END max_depth=35, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time=   4.1s\n",
      "[CV] END max_depth=35, min_samples_leaf=5, min_samples_split=2, n_estimators=150; total time=   3.4s\n",
      "[CV] END max_depth=35, min_samples_leaf=5, min_samples_split=2, n_estimators=150; total time=   3.3s\n",
      "[CV] END max_depth=35, min_samples_leaf=5, min_samples_split=2, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=35, min_samples_leaf=5, min_samples_split=5, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=35, min_samples_leaf=5, min_samples_split=5, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=35, min_samples_leaf=5, min_samples_split=5, n_estimators=150; total time=   3.1s\n",
      "[CV] END max_depth=35, min_samples_leaf=5, min_samples_split=10, n_estimators=150; total time=   3.0s\n",
      "[CV] END max_depth=35, min_samples_leaf=5, min_samples_split=10, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=35, min_samples_leaf=5, min_samples_split=10, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=35, min_samples_leaf=10, min_samples_split=2, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=35, min_samples_leaf=10, min_samples_split=2, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=35, min_samples_leaf=10, min_samples_split=2, n_estimators=150; total time=   2.8s\n",
      "[CV] END max_depth=35, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=   2.9s\n",
      "[CV] END max_depth=35, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=35, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=35, min_samples_leaf=10, min_samples_split=10, n_estimators=150; total time=   2.9s\n",
      "[CV] END max_depth=35, min_samples_leaf=10, min_samples_split=10, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=35, min_samples_leaf=10, min_samples_split=10, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=150; total time=   3.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=150; total time=   4.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=150; total time=   4.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=150; total time=   3.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=150; total time=   3.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=150; total time=   3.9s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time=   3.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time=   3.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=10, n_estimators=150; total time=   3.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=5, min_samples_split=2, n_estimators=150; total time=   3.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=5, min_samples_split=2, n_estimators=150; total time=   3.6s\n",
      "[CV] END max_depth=None, min_samples_leaf=5, min_samples_split=2, n_estimators=150; total time=   3.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=150; total time=   3.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=150; total time=   3.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=5, min_samples_split=5, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=150; total time=   3.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=150; total time=   3.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=5, min_samples_split=10, n_estimators=150; total time=   3.5s\n",
      "[CV] END max_depth=None, min_samples_leaf=10, min_samples_split=2, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=10, min_samples_split=2, n_estimators=150; total time=   2.8s\n",
      "[CV] END max_depth=None, min_samples_leaf=10, min_samples_split=2, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=10, min_samples_split=5, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=150; total time=   2.7s\n",
      "[CV] END max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=150; total time=   2.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END max_depth=None, min_samples_leaf=10, min_samples_split=10, n_estimators=150; total time=   2.6s\n",
      "Best Hyperparameters: {'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "rfc = RandomForestClassifier(n_jobs=-1,random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    \"n_estimators\": [150],\n",
    "    \"max_depth\": [30, 35, None],\n",
    "    'min_samples_leaf':[2, 5, 10],\n",
    "    \"min_samples_split\": [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV (5-fold cross-validation)\n",
    "grid_search = GridSearchCV(rfc, param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Get best parameters and train the final model\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "#Best Hyperparameters: {'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48e38cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7518721917124314\n",
      "Classification Report:\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "          Back-End Developer       0.75      0.52      0.61      1903\n",
      "               Data Engineer       0.40      0.26      0.31       180\n",
      "Data Scientist/ML Specialist       0.64      0.67      0.65       196\n",
      "        Full-Stack Developer       0.76      0.92      0.83      3403\n",
      "            Mobile Developer       0.84      0.72      0.78       327\n",
      "\n",
      "                    accuracy                           0.75      6009\n",
      "                   macro avg       0.68      0.62      0.64      6009\n",
      "                weighted avg       0.75      0.75      0.74      6009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train RFC with best params\n",
    "best_params = {'max_depth': 30, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n",
    "\n",
    "best_rfc = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_rfc.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = best_rfc.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a9212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
